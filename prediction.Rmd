---
title: "Stock Price Prediction"
author: "Xiaochun Fan"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This project will be attempting to predict stock price moving direction based on historical stock data. Historic data will be gathered from Yahoo Finance. Stock pricing data is extremly noisy and not an easy task to predict, therefore this project will just focus on general methods instead of results.

```{r include=FALSE}
# install required packages
packages <- c('dplyr', 'caret', 'tidyquant', 'ggplot2', 'zoo', 'roll', 'parallel', 'doParallel')
for (pack in packages) {
  if (!require(pack, character.only = TRUE)) install.packages(pack)
  library(pack, character.only = TRUE)
}
```

## Step 1: Load and Transform Data

Stock pricing data can be loaded directly from Yahoo Finance using 'getSymbols' function from 'tidyquant' package.

```{r}
# loading stock pricing data
stock_raw <- getSymbols(Symbols = 'SPY', auto.assign = FALSE) %>%
  `colnames<-`(c('Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted_Close'))
```

We can plot historic SPY price data:

```{r, echo=FALSE}
plot(stock_raw$Close, type = 'l')
```

This project will be focusing on predicting directions, a binary value of 1 or 0, with 1 meaning stock moves up. Therefore, the absolute value of the stock price isn't that important, it's the relationship between pricing data that we will be focusing on.

The following code create relationships between pricing changes and create prediction targets.

```{r}
# Compute price changes and create prediction target (y)
stock <- stock_raw %>%
  data.frame() %>%
  mutate(change = Close - Open, change_pct = change * 100 / Open,
         high_pct = (High - Open) * 100 / Open,
         low_pct = (Low - Open) * 100 / Open) %>%
  mutate(direction = ifelse(change_pct > 0, 1, 0), y = lead(direction),
         y = factor(y)) %>%
  mutate(direction = NULL) # remove the known "direction" column.
```

Here's a snippet of the transformed data:

```{r}
head(stock)
```

## Step 2: Spliting Data

Since stock pricing is a time-series data, we want to split the data into training and testing sets before creating more predictors. A lot of the predictors will be based on averaging certain period of past data, therefore if predictors are created before spliting data, it's possible that information from training data could leak into testing data due to averaging.

The code below splits data into training set for model training, ensemble_test set for ensemble models and select the best ensembling method, and final_test set for final evaluation.

```{r}
# split data before creating predictors
train <- stock[1:3000, ]
ensemble_test <- stock[3001:4000, ]
final_test <- stock[4001:nrow(stock), ]
```

## Step 3: Generator Predictors

Directly using daily stock data as predictors probably won't be very helpful, since it lacks relationship to past data. Therefore in this step, we will generate predictors based on rolling averages and recent high/lows of historic data from certain time periods.

```{r, echo=FALSE}
# Compute rolling price averages and recent high/lows
moving_avg_n <- c(10, 20, 40, 80)

rolling <- function(x){
  data <- x
  moving_avg <- list()
  for (n in moving_avg_n){
    # Rolling average price changes
    col_name <- paste('sma', n, sep = '_')
    moving_avg[[col_name]] <- (data$Close - SMA(data$Close, n = n)) * 100 / data$Close
    col_name <- paste('ema', n, sep = '_')
    moving_avg[[col_name]] <- (data$Close - EMA(data$Close, n = n)) * 100 / data$Close
    col_name <- paste('evwma', n, sep = '_')
    moving_avg[[col_name]] <- (data$Close - EVWMA(data$Close, data$Volume, n = n)) * 100 / data$Close
    
    # Rolling min/max
    col_name <- paste('high', n, sep = '_')
    moving_avg[[col_name]] <- (data$Close - rollmax(data$High, n, fill = NA, align = 'right')) / data$Close
    col_name <- paste('low', n, sep = '_')
    moving_avg[[col_name]] <- (data$Close - -rollmax(-data$Low, n, fill = NA, align = 'right')) / data$Close
    
    # Rolling volume
    col_name <- paste('vol', n, sep = '_')
    moving_avg[[col_name]] <- (data$Volume - rollmean(data$Volume, n, fill = NA, align = 'right')) / data$Volume
    
    # Rolling standard deviation (volatility)
    col_name <- paste('sd', n, sep = '_')
    moving_avg[[col_name]] <- roll_sd(data$change_pct, n)
  }
  x <- x %>%
    cbind(moving_avg)
  return(x)
}

train <- rolling(train)
final_test <- rolling(final_test)
ensemble_test <- rolling(ensemble_test)
```

Next, we remove the columns containing absolute values of the stock pricing data, and only leave relavent pricing data as predictors.

```{r echo=FALSE}
# Filter out predictor columns
filt_columns <- function(x){
  x <- x %>%
    select(-c('Open', 'Close', 'High', 'Low', 'Volume', 'Adjusted_Close')) %>%
    na.omit()
  return(x)
}

train <- filt_columns(train)
final_test <- filt_columns(final_test)
ensemble_test <- filt_columns(ensemble_test)
```

Here's a snippet of the processed data:
```{r}
head(train)
```

## Step 4: Model Training

This step will train some models using training dataset.

This step will take some time to run, depends on computer performance.

This optional code below can use multiple cores to speed up training process:

```{r}
# setup parallel computation for multi-core computers
if (detectCores() > 1){
  num_core <- detectCores() - 1
  pl <- makeCluster(num_core)
  registerDoParallel(pl)
} else {
  registerDoSEQ()
}
```

Train some models:

```{r message=FALSE, warning=FALSE}
# setup training control
control <- trainControl(method = 'timeslice', initialWindow = 500, 
                        horizon = 300, fixedWindow = FALSE, skip = 49)

# train some models
fit <- list()
fit[['knn']] <- train(y ~ ., data = train, trControl = control, method = 'knn', 
                      tuneGrid = data.frame(k = seq(1, 15, 2)))
fit[['rf']] <- train(y ~ ., data = train, trControl = control, method = 'rf', 
                     tuneGrid = data.frame(mtry = seq(2, ncol(train), 2)))
fit[['glm']] <- train(y ~ ., data = train, trControl = control, method = 'glm')
fit[['rpart']] <- train(y ~ ., data = train, trControl = control, method = 'rpart', 
                        tuneGrid = data.frame(cp = seq(0.005, 0.05, 0.005)))
fit[['xgbTree']] <- train(y ~ ., data = train, trControl = control, method = 'xgbTree')
fit[['earth']] <- train(y ~ ., data = train, trControl = control, method = 'earth')
```

## Step 5: Ensemble Models

This step will compare prediction results from 6 models and using the majority to decide which prediction to be used.

This code below creates a function that takes in the number of models to use, and output the prediction results.

For accuracy metrics, we should focus more on precision, instead of overall accuracy. Also, only using accuracy to test performance may not work as intended, because the magnitude of price changes were ignored.

Therefore, this function also calculates the dollar amount the stock has changed during the testing data period, and compared to the predicted result ('portfolio' vs 'buy_and_hold' metrics).

```{r}
result <- function(x, test_data){
  pred <- list()
  for (f in fit){
    pred[[f$method]] <- predict(f, test_data)
  }
  
  ensemble <- pred %>%
    data.frame() %>%
    mutate(count = rowSums(. == 1)) %>%
    mutate(y_hat = ifelse(count >= x, 1, 0),
           y_hat = lag(y_hat),
           y_hat = factor(y_hat, levels = levels(test_data$y))) %>%
    na.omit()
  
  test_data <- test_data %>%
    mutate(y = lag(y)) %>%
    na.omit()
  
  ensemble <- cbind(test_data, ensemble) %>%
    select(y, y_hat, change)
  conf <- confusionMatrix(ensemble$y_hat, ensemble$y, positive = '1')
  ensemble <- ensemble %>% 
    mutate(y_hat = as.numeric(as.character(y_hat))) %>%
    summarise(portfolio = sum(ensemble$change * y_hat), 
              buy_and_hold = sum(ensemble$change),
              precision = conf$byClass['Precision'],
              sensitivity = conf$byClass['Sensitivity'],
              overall_accu = conf$overall['Accuracy'])
  
  return(ensemble)
}
```

Next, we will test which ensemble parameter performs best:

```{r}
ensemble_n <- seq(1, length(fit), 1)
lapply(ensemble_n, function(x) result(x, ensemble_test))
```

## Step 6: Final Testing

After selecting ensemble parameters (5, in this case), we will test it on the final testing dataset, using code below:

```{r}
# Final testing
result(5, final_test)
```

## Conclusion

It's clear the accuracy is low, and seems very inconsistent (performance from ensemble_test dataset do not translate well to final_test dataset). The prediction method can potentially be improved by selecting / excluding some models, adding more predictors, adding pricing data from other stocks/instruments, and using higher resolution data.
